{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EGRMGMT 590.10 Homework 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Due at 6:15pm ET on Thursday, March 21*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ There are 9 exercises in total. Exercise 1 through 3 require coding. Your grades are based on the accuracy of the results, not the code execution speed.\n",
    "  + Exercise 1: 1 point.\n",
    "  + Exercise 2 and 3: each is worth 3 points. \n",
    "  + Exercise 4 to 9: each is worth 0.5 points. \n",
    "+ Exercise 1 through 3 require generating model parameters using code you built **from scratch** without relying on existing modules with functions or capabilities to \"plug-in and populate\" parameters for those models.\n",
    "+ As the coding exercises require matrix manipulation, familiarize yourself with the following functions and methods:\n",
    "  + numpy.linalg.inv\n",
    "  + numpy.ndarray.dot\n",
    "  + numpy.dot\n",
    "  + numpy.ndarray.T   \n",
    "+ Exercises are independent with each other. Feel free to work on them in the order you prefer.\n",
    "+ Submit the .ipynb file to Sakai before 6:15pm ET on Thursday, March 21.\n",
    "+ Assignments handed in late will lose 1 point every 24-hour window after 6:15pm ET on Thursday, March 21."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Information:**\n",
    "\n",
    "We'll be working on three datasets attached to Assignment 4 on Sakai. Each coding exercise requires a different dataset:\n",
    "+ Exercise 1: 'Assignment4_SampleData_Ex1.csv'\n",
    "+ Exercise 2: 'Assignment4_SampleData_Ex2.csv'\n",
    "+ Exercise 3: 'Assignment4_SampleData_Ex3.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-parameter Linaer Regression Modle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T16:13:58.816852Z",
     "start_time": "2019-03-19T16:13:56.860243Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import copy as copy\n",
    "import os\n",
    "import sys  as sys\n",
    "import math as math\n",
    "plt.rc('figure', figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T17:29:23.449688Z",
     "start_time": "2019-03-19T17:29:23.328854Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1a21d6fb00>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1 = pd.read_csv('Dummy_data.csv')\n",
    "theta = []\n",
    "m = df1['X'].size\n",
    "bias_vector = np.ones((m,1))\n",
    "x_ary = np.array(df1['X'])\n",
    "x_ary = np.reshape(x_ary, (m,1))\n",
    "x_ary = np.append(x_ary, bias_vector, axis = 1)\n",
    "X_transpose = np.transpose(x_ary)\n",
    "# Calculating theta\n",
    "theta = np.linalg.inv(X_transpose.dot(x_ary))\n",
    "theta = theta.dot(X_transpose)\n",
    "theta = theta.dot(df1['Y'])\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(2, 2, 1)\n",
    "ax1.scatter(df1.iloc[:,1] , df1.iloc[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T20:29:00.902220Z",
     "start_time": "2019-03-18T20:29:00.897964Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.93431811, 5.21509616])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T19:23:07.922738Z",
     "start_time": "2019-03-18T19:23:07.918147Z"
    }
   },
   "source": [
    "### My slope = 8.93431811\n",
    "### My intercept = 5.21509"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T20:29:40.203153Z",
     "start_time": "2019-03-18T20:29:40.196740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.93431811]\n",
      "5.215096157546743\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "df1_x = np.array(df1['X'])\n",
    "reg = LinearRegression().fit(df1_x.reshape(-1,1), df1[\"Y\"])\n",
    "print(reg.coef_)\n",
    "print(reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn's result is the same as mine\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building up two-variable Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T03:38:43.760877Z",
     "start_time": "2019-03-19T03:38:43.089209Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4.91025075, 5.82932544, 9.10279851])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('Dummy_data.csv')\n",
    "Y = np.array(df2['Y'])\n",
    "m = Y.size\n",
    "X1 = np.array(df2['X1'])\n",
    "X1 = X1.reshape(m,1)\n",
    "X2 = np.array(df2['X2'])\n",
    "X2 = X2.reshape(m,1)\n",
    "X = np.append(X1, X2, axis = 1)\n",
    "def calc_cost(theta, X, Y):\n",
    "    cost = 0\n",
    "    m = Y.size\n",
    "    for i in range(m):\n",
    "        cost_i = ((np.transpose(theta)).dot(X[i]) - Y[i])**2\n",
    "        cost = cost + cost_i\n",
    "    cost = cost / m\n",
    "    return cost\n",
    "\n",
    "def BGD(X, Y, eta = 0.01, n_rounds = 10000, tol = 10**(-10), random_state = 42):\n",
    "    times = 0\n",
    "    cost_pre = sys.maxsize\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    m = X.shape[0]\n",
    "    bias_vector = np.ones((m,1))\n",
    "    X_apd = np.append(bias_vector, X, axis = 1)\n",
    "    X_T = np.transpose(X_apd)\n",
    "    theta = np.random.rand(X_apd.shape[1])\n",
    "    while (times < n_rounds):\n",
    "        grd_vector = (2/m) * X_T.dot(X_apd.dot(theta) - Y)\n",
    "        theta = theta - eta * grd_vector\n",
    "        cost_curr = calc_cost(theta, X_apd, Y)\n",
    "        if (abs(cost_pre - cost_curr) < tol):\n",
    "            break\n",
    "        cost_pre = cost_curr\n",
    "        times = times + 1\n",
    "    return theta\n",
    "    \"\"\"\n",
    "    Returns the intercept and predictor coefficients for a multiple linear regression model using Batch Gradient Descent \n",
    "    with MSE as the cost function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: a numpy ndarray with the independent variables\n",
    "    Y: a numpy ndarray with the dependent variable\n",
    "    eta: the learning rate, e.g. 0.1, 0.01, 0.001 etc.\n",
    "    n_rounds: number of passes over the data, e.g. 100, 1000, 10000 etc.\n",
    "    tol: the stopping criteria, i.e. previous MSE - current MSE < tol\n",
    "    random_state: a random state to make the randomness deterministic\n",
    "    \n",
    "    Examples\n",
    "    ----------\n",
    "    BGD(X, Y)\n",
    "    BGD(X, Y, eta = 0.01)\n",
    "    \n",
    "    Notes\n",
    "    ----------\n",
    "    The parameters are returned in a numpy ndarray.\n",
    "    \n",
    "    \"\"\"\n",
    "BGD(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T01:23:41.547432Z",
     "start_time": "2019-03-19T01:23:41.541876Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept =  4.910610036450052\n",
      "coeffient =  [5.82913734 9.10275877]\n"
     ]
    }
   ],
   "source": [
    "reg1 = LinearRegression().fit(X, Y)\n",
    "print(\"intercept = \", reg1.intercept_)\n",
    "print(\"coeffient = \", reg1.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My result is close to the sklearn's result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building up a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T18:41:38.257843Z",
     "start_time": "2019-03-19T18:32:02.723910Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.7835683 , 3.00883649, 3.96489009, 2.49815872])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = pd.read_csv('Assignment4_SampleData_Ex3.csv')\n",
    "Y = np.array(df3['Y'])\n",
    "m = Y.size\n",
    "X1 = np.array(df3['X1']).reshape(m,1)\n",
    "X2 = np.array(df3['X2']).reshape(m,1)\n",
    "X3 = np.array(df3['X3']).reshape(m,1)\n",
    "X_12 = np.append(X1, X2, axis = 1)\n",
    "X = np.append(X_12, X3, axis = 1)\n",
    "print(X.shape)\n",
    "def calc_cost(theta, X, Y):\n",
    "    cost = 0\n",
    "    m = Y.size\n",
    "    for i in range(m):\n",
    "        omega = 1 / (1 + np.exp(-1 * (np.transpose(theta)).dot(X[i])))\n",
    "        cost_i = Y[i]*math.log(omega) + (1-Y[i])*math.log(1-omega)\n",
    "        cost = cost + cost_i\n",
    "    cost = cost / m * -1\n",
    "    return cost\n",
    "\n",
    "def BGD_LogisticRegression(X, Y, eta = 0.001, n_rounds = 100000, tol = 10**(-10), random_state = 42):\n",
    "    m = Y.size\n",
    "    times = 0\n",
    "    cost_pre = sys.maxsize\n",
    "    \n",
    "    bias_vector = np.ones((m,1))\n",
    "    X_apd = np.append(X, bias_vector, axis = 1)\n",
    "    theta = np.random.rand(X_apd.shape[1])\n",
    "    while (times < n_rounds):\n",
    "        g = 1 / (1 + np.exp(-1 * X_apd.dot(theta))) \n",
    "        grd_vector = 1 / m * (np.transpose(X_apd)).dot(g - Y)\n",
    "        theta = theta - eta*grd_vector\n",
    "        cost_curr = calc_cost(theta, X_apd, Y)\n",
    "        if (abs(cost_pre - cost_curr) < tol):\n",
    "            print(\"cost_curr = \", cost_curr)\n",
    "            print(\"cost_pre = \", cost_pre)\n",
    "            break    \n",
    "        cost_pre = cost_curr\n",
    "        times = times + 1\n",
    "    return theta\n",
    "    \"\"\"\n",
    "    Returns the intercept and predictor coefficients for a binary logistic regression model using Batch Gradient Descent \n",
    "    with the log loss as the cost function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: a numpy ndarray with the independent variables\n",
    "    Y: a numpy ndarray with the dependent binary categorical variable\n",
    "    eta: the learning rate, e.g. 0.1, 0.01, 0.001 etc.\n",
    "    n_rounds: number of passes over the data, e.g. 100, 1000, 10000 etc. \n",
    "    tol: the stopping criteria, i.e. previous log loss - current log loss < tol\n",
    "    random_state: a random state to make the randomness deterministic\n",
    "    \n",
    "    Examples\n",
    "    ----------\n",
    "    BGD_LogisticRegression(X, Y)\n",
    "    BGD_LogisticRegression(X, Y, eta = 0.001)\n",
    "    \n",
    "    Notes\n",
    "    ----------\n",
    "    The parameters are returned in a numpy ndarray.\n",
    "    \n",
    "    \"\"\"\n",
    "BGD_LogisticRegression(X, Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My theta is(0.7835683 , 3.00883649, 3.96489009)\n",
    "My intercept is 2.49815872"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T02:11:19.225923Z",
     "start_time": "2019-03-20T02:11:19.210613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.958\n",
      "[[0.88944338 2.9908795  4.01841388]]\n",
      "[2.38651297]\n",
      "0.9732142857142857\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=42, solver='lbfgs', multi_class='multinomial').fit(X, Y)\n",
    "sk_Y = clf.predict(X)\n",
    "print(clf.score(X, Y))\n",
    "print(clf.coef_)\n",
    "print(clf.intercept_)\n",
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(sk_Y, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T02:11:31.790682Z",
     "start_time": "2019-03-20T02:11:31.785141Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    q0 = 0.7835683\n",
    "    q1 = 3.00883649\n",
    "    q2 = 3.96489009\n",
    "    q3 = 2.49815872\n",
    "    \n",
    "    x1 = x[1]\n",
    "    x2 = x[2]\n",
    "    x3 = x[3]\n",
    "\n",
    "    p = 1 / (1 + np.exp(-1*(q3 + x1*q0 + x2*q1 + x3*q2)))\n",
    "    return p > 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T02:13:11.468626Z",
     "start_time": "2019-03-20T02:13:11.162581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "TN =189,FP = 31\n",
      "FN =11,TP = 769\n",
      "F1 score =  0.9734177215189873\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9734177215189873"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = []\n",
    "result = []\n",
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "\n",
    "for i in range(df3['Y'].size):\n",
    "    prediction.append(predict(df3.iloc[i]))\n",
    "    result.append(df3.iloc[i][0])\n",
    "    if ((prediction[i] == 1) & (df3.iloc[i][0] == 1)) :\n",
    "        TP = TP + 1\n",
    "    elif ((prediction[i] == 1) & (df3.iloc[i][0] == 0)):\n",
    "        FP = FP + 1\n",
    "    elif ((prediction[i] == 0) & (df3.iloc[i][0] == 0)):\n",
    "        TN = TN + 1\n",
    "    else:\n",
    "        FN = FN + 1\n",
    "print(\"TN =\" + str(TN) + \",FP = \" + str(FP))\n",
    "print(\"FN =\" + str(FN) + \",TP = \" + str(TP))\n",
    "F1 = 2*TP / (2*TP + FP + FN)\n",
    "print(\"F1 score = \", F1)\n",
    "f1_score(prediction, Y)\n",
    "\n",
    "#ax2 = fig.add_subplot(2,2,2)\n",
    "#plt.scatter(prediction,result)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T17:40:39.985272Z",
     "start_time": "2019-03-19T17:40:39.971170Z"
    }
   },
   "source": [
    "My F1 score is close to sklearn's F1 score, the difference is negligible"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "notify_time": "30"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
